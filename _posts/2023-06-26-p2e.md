---
title: "Plan2Explore"
author: Michele
excerpt: "P2E - an exploration algorithm based on Dreamer"
tags:
published: true
layout: post
current: post
cover: assets/images/p2e_cover.png
navigation: True
class: post-template
subclass: 'post'
---

### The P2E algorithm
In [DreamerV1]({{site.baseurl}}/2023/06/16/dreamer_v1.html){:target="_blank"} we saw that the world model is reilably learned by the world model. In [[1]](#ref-1){: .ref-link} an ad hoc method is proposed to learn the world model by exploring the environment and to increase the generalization of the world model, so that different tasks can be learned with less effort.

The main idea is to replace the reward of the task with an *intrinsic reward* that estimates the level of novelty of a state of the environment. The newer the state, the more intrinsic reward is given to the agent. If the model vistis always the same state, the intrinsic reward will be low, so the agent is pushed to visit states it has never encountered before.

#### The ensembles and the intrinsic reward
Now the question that arises is, how does one compute the novelty of a state? Sekar et al. introduced the ensembles ([Figure 1](#fig-ens){: .fig-link}), i.e., several MLPs initialized with different weights, that try to predict the embedding of the next observations (provided by the environment and embedded by the encoder). The more similar the predictions of the ensembles are, the lower the novelty of the state. Indeed, novelty comes from the disagreement of the ensembles, and the models will converge towards more similar predictions for states that are visited many times.

<img id="fig-ens" src="{{site.baseurl}}/assets/images/p2e/ensemble.png" style="margin-bottom: 10px; max-width: 300px;" /><image_caption style="margin-bottom:28px; width: 100%; text-align: center; display: block;">Figure 1: How the ensemble works. <a href="#ref-1" class="ref-link">[1]</a>. The ensemble is an MLP that tries to predict the embedding of the next observation. Several ensembles are exploited to compute the novelty of a state: the greater the disagreement between them, the newer the state. It is possible to notice from this picture that the stochastic state is the one computed by the transition model.</image_caption>

A note should be made about the prediction of the embedding of the next observation: the ensemble takes as input the latent state, composed by the predicted stochastic state (computed by the transition model) and the recurrent state, and the performed action (the one that led to the latent state in input). It is necessary to use the stochastic state predicted by the transition model because during the imagination, the agent has no observations, so the ensembles must be trained on the same kind of data.



#### Zero-shot vs Few-shot

With the world model trained to explore the environment, one can test:

* in a *zero-shot* setting whether the exploration experience is useful to learn the task at hand: given the task rewards (the ones that the environment returns at every step and that represent the tasked to be solved) obtained during the exploration, is the agent able to learn a behaviour that also solves the task?
* in *few-shot* setting whether finetuning the agent with few interactions with the environment (150k steps tipically) helps to improve the performances further. In this setting the agent will collect new experiences with the intent to maximize its performance in solving the task: it is no more interested in exploring the environment.

Both settings can be tested on different environments than the one explored to assess further the generaliztion capabilities of the agent.

### Experiments

Check out [our implementation](https://github.com/Eclectic-Sheep/sheeprl/tree/main/sheeprl/algos/dreamer_v1){:target="_blank"}.

### References
<div id="ref-1" class="ref" style="margin-bottom: 10px">[1] Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. <a target="_blank" href="https://arxiv.org/abs/2005.05960">Planning to Explore via Self-Supervised World Models</a> <i>CoRR</i>, abs/2005.05960</div>

<div id="ref-2" class="ref" style="margin-bottom: 10px">[2] Danijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and
Mohammad Norouzi. 2019. <a target="_blank" href="https://arxiv.org/abs/1912.01603">Dream to Control: Learning Behaviors by Latent Imagination</a> <i>CoRR</i>, abs/1912.01603</div>

<div id="ref-3" class="ref">[3] Danijar Hafner, Timothy P. Lillicrap, Ian Fischer, Ruben
Villegas, David Ha, Honglak Lee, and James Davidson. 2018 <a target="_blank" href="https://arxiv.org/abs/1811.04551">Learning Latent Dynamics for Planning from Pixels</a> <i>CoRR</i>, abs/1811.04551</div>