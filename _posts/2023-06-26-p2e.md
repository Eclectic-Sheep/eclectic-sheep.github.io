---
title: "Plan2Explore"
author: Michele
excerpt: "P2E - an exploration algorithm based on Dreamer"
tags:
published: true
layout: post
current: post
cover: assets/images/p2e_cover.png
navigation: True
class: post-template
subclass: 'post'
---

### The P2E algorithm
In [DreamerV1]({{site.baseurl}}/2023/06/16/dreamer_v1.html){:target="_blank"} [[2]](#ref-2){: .ref-link} we saw that the world model is reilably learned by the world model. In [[1]](#ref-1){: .ref-link} an ad hoc method is proposed to learn the world model by exploring the environment and to increase the generalization of the world model, so that different tasks can be learned with less effort.

The main idea is to replace the reward of the task with an *intrinsic reward* that estimates the level of novelty of a state of the environment. The newer the state, the more intrinsic reward is given to the agent. If the model vistis always the same state, the intrinsic reward will be low, so the agent is pushed to visit states it has never encountered before.

#### The ensembles and the intrinsic rewards
Now the question that arises is, how does one compute the novelty of a state? Sekar et al. introduced the ensembles ([Figure 1](#fig-ens){: .fig-link}), i.e., several MLPs initialized with different weights, that try to predict the embedding of the next observations (provided by the environment and embedded by the encoder). The more similar the predictions of the ensembles are, the lower the novelty of the state. Indeed, novelty comes from the disagreement of the ensembles, and the models will converge towards more similar predictions for states that are visited many times.

<img id="fig-ens" src="{{site.baseurl}}/assets/images/p2e/ensemble.png" style="margin-bottom: 10px; max-width: 300px;" /><image_caption style="margin-bottom:28px; width: 100%; text-align: center; display: block;">Figure 1: How the ensemble works. <a href="#ref-1" class="ref-link">[1]</a>. The ensemble is an MLP that tries to predict the embedding of the next observation. Several ensembles are exploited to compute the novelty of a state: the greater the disagreement between them, the newer the state. It is possible to notice from this picture that the stochastic state is the one computed by the transition model.</image_caption>

A note should be made about the prediction of the embedding of the next observation: the ensemble takes as input the latent state, composed by the predicted stochastic state (computed by the transition model) and the recurrent state, and the performed action (the one that led to the latent state in input). It is necessary to use the stochastic state predicted by the transition model because during the imagination, the agent has no observations, so the ensembles must be trained on the same kind of data.

Now we need to measure the level of disagreement between the ensembles: in the solution proposed in [[1]](#ref-1){: .ref-link}, the disagreement is given by the *intrinsic reward* ($\text{ir}$), i.e., the variance of the outputs of the ensembles.

<div class="formula-wrapper">
    $$ \text{ir} = \frac{1}{K - 1} \sum_{k} \mu_{k}(s_t, h_t, a_{t-1}) - \mu^\prime $$
</div>

Where $K$ is the number of ensembles, $\mu_{k}(s_t, h_t, a_{t-1})$ is the output of the $k$-th ensemble at time step $t$, and $\mu^\prime = \frac{1}{K} \sum_{k} \mu_{k}(s_t, h_t, a_{t-1})$ is the mean of the outputs of the ensembles.
These intrinsic rewards are computed at each gradient step during the imagination phase.

#### Zero-shot vs Few-shot

With the world model trained to explore the environment, one can test:

* in a *zero-shot* setting whether the exploration experience is useful to learn the task at hand: given the task rewards (the ones that the environment returns at every step and that represent the tasked to be solved) obtained during the exploration, is the agent able to learn a behaviour that also solves the task?
* in *few-shot* setting whether finetuning the agent with few interactions with the environment (150k steps tipically) helps to improve the performances further. In this setting the agent will collect new experiences with the intent to maximize its performance in solving the task: it is no more interested in exploring the environment.

Both settings can be tested on different environments than the one explored to assess further the generaliztion capabilities of the agent.

### Experiments

We have conducted some experiments on the CarRacing environment to assess the generalization capabilities of P2E. The results are shown in [Figure 2](#fig-few){: .fig-link} and [Figure 3](#fig-zero){: .fig-link}.

<div class="blog-img-wrapper video" id="fig-few" style="margin-bottom: 10px;">
    <div class="video-wrapper">
        <video muted autoplay loop controls>
            <source src="/assets/videos/p2e/zero-shot-42.mp4" type="video/mp4" />
        </video><image_caption style="margin-bottom:28px; width: 100%; text-align: center; display: block;">Figure 9.a: Test without finetuning on the track number 42.</image_caption>
    </div>
    <div class="video-wrapper">
        <video muted autoplay loop controls>
            <source src="/assets/videos/p2e/few-shot-42.mp4" type="video/mp4" />
        </video><image_caption style="margin-bottom:28px; width: 100%; text-align: center; display: block;">Figure 9.b: Test with finetuning (few-shot setting) on the track number 42.</image_caption>
    </div>
</div><image_caption style="margin-bottom:28px; width: 100%; text-align: center; display: block;">Figure 2: We trained Dreamer with the P2E algorithm on the track number 42 of the CarRacing environment. We trained also the agent in a zero-shot setting on the track number 42. Then we fine-tuned the agent on the same track (few-shot setting), achieving better results. </image_caption>

<div class="blog-img-wrapper video" id="fig-zero" style="margin-bottom: 10px;">
    <div class="video-wrapper">
        <video muted autoplay loop controls>
            <source src="/assets/videos/p2e/zero-shot-512.mp4" type="video/mp4" />
        </video><image_caption style="margin-bottom:28px; width: 100%; text-align: center; display: block;">Figure 9.a: Test without finetuning on the unseen track number 512.</image_caption>
    </div>
    <div class="video-wrapper">
        <video muted autoplay loop controls>
            <source src="/assets/videos/p2e/zero-shot-1024.mp4" type="video/mp4" />
        </video><image_caption style="margin-bottom:28px; width: 100%; text-align: center; display: block;">Figure 9.b: Test without finetuning on the unseen track number 1024.</image_caption>
    </div>
    <div class="video-wrapper">
        <video muted autoplay loop controls>
            <source src="/assets/videos/p2e/zero-shot-2030.mp4" type="video/mp4" />
        </video><image_caption style="margin-bottom:28px; width: 100%; text-align: center; display: block;">Figure 9.c: Test without finetuning on the unseen track number 2030.</image_caption>
    </div>
    <div class="video-wrapper">
        <video muted autoplay loop controls>
            <source src="/assets/videos/p2e/zero-shot-2048.mp4" type="video/mp4" />
        </video>
        <image_caption style="margin-bottom:28px; width: 100%; text-align: center; display: block;">Figure 9.d: Test without finetuning on the unseen track number 2048.</image_caption>
    </div>
</div><image_caption style="margin-bottom:28px; width: 100%; text-align: center; display: block;">Figure 3: We trained Dreamer with the P2E algorithm on the track number 42 of the CarRacing environment. We trained also the agent in a zero-shot setting on the track number 42. Then we tested the agent on different tracks (never seen before), achieving good results.</image_caption>

Check out [our implementation](https://github.com/Eclectic-Sheep/sheeprl){:target="_blank"}.

### References
<div id="ref-1" class="ref" style="margin-bottom: 10px">[1] Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. <a target="_blank" href="https://arxiv.org/abs/2005.05960">Planning to Explore via Self-Supervised World Models</a> <i>CoRR</i>, abs/2005.05960</div>

<div id="ref-2" class="ref" style="margin-bottom: 10px">[2] Danijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and
Mohammad Norouzi. 2019. <a target="_blank" href="https://arxiv.org/abs/1912.01603">Dream to Control: Learning Behaviors by Latent Imagination</a> <i>CoRR</i>, abs/1912.01603</div>