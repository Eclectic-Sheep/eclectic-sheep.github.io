---
title: "From 0 to RL with SheepRL"
author: Federico
excerpt: "In the last few months we have achieved a lot with SheepRL, solving difficult environments like Minecraft, Diambra, Crafter and more. In this blogpost we want to provide an overview and share what has been accomplished, as well as outline our future directions."
tags:
published: true
layout: post
current: post
cover: assets/images/dreamer_v3_cover.png
navigation: True
class: post-template
subclass: 'post'
---

### SheepRL: an overview

In the rapidly evolving landscape of artificial intelligence and machine learning, SheepRL has emerged as a noteworthy contender in the field of Reinforcement Learning (RL) frameworks, demonstrating remarkable growth and adaptability across various dimensions.

One of the standout features of SheepRL is its versatility in handling different types of actions. Whether it's discrete, multi-discrete, or continuous actions, SheepRL is designed to support them all. This flexibility empowers developers to tackle a wide range of RL problems, making it a valuable tool for researchers and practitioners alike.

What truly sets SheepRL apart is its commitment to staying at the cutting edge of RL algorithms. The inclusion of state-of-the-art algorithms like [Dreamer-V3](https://eclecticsheep.ai/2023/08/10/dreamer_v3.html), demonstrates SheepRL's dedication to providing users with the most advanced and effective tools available in the field. Dreamer-V3, in particular, showcases the framework's capability to simulate and learn from imagined experiences, a feature that opens up exciting possibilities for training RL agents.

SheepRL's fully distributed nature, made possible by [Lightning Fabric](lightning.ai/docs/fabric/stable/), ensures that RL practitioners can harness the power of distributed computing seamlessly. This distributed approach enables more efficient training of RL models, making it suitable for tackling complex real-world problems that demand substantial computational resources.

Moreover, SheepRL's adaptability is further enhanced by its integration with [Hydra](hydra.cc). This integration simplifies the process of configuring and customizing RL experiments, making it user-friendly and accessible to a wide range of developers, from beginners to experts. The framework's emphasis on clarity and ease of use is a testament to its commitment to fostering a welcoming and productive RL development environment.

### Dreamer-V3 beats them all!

To demonstrate the power and the flexibility offered by our framework we have run experiments with different environments and a single algorithm, namely [Dreamer-V3](https://arxiv.org/abs/2301.04104), with a **single set of hyperparameters** for every environment.

#### Diambra

[Diambra](https://diambra.ai/) is a competition platform that allows you to submit your agents and compete with other coders around the globe in epic video games tournaments! Featuring a collection of [high-quality environments](https://docs.diambra.ai/envs/#available-games) for Reinforcement Learning research and experimentation, it provides a standard interface to popular arcade emulated video games, offering a Python API fully compliant with OpenAI Gym/Gymnasium format, that makes its adoption smooth and straightforward. 

We have trained a Dreamer-V3 agent in the [Dead or Alive++](https://docs.diambra.ai/envs/games/doapp/) environment obtaining the [best score in the global leaderboard](https://diambra.ai/leaderboard). Moreover, by looking at the actions' distribution, we have noted that the agent was able to win the game in the hardest difficulty by simply standing down and throwing kicks.

<div class="video-wrapper">
    <video muted autoplay loop controls>
        <source src="/assets/videos/sheepRL_dreamerV3_diambra.mp4" type="video/mp4" />
    </video>
    <image_caption style="margin-bottom:28px; width: 100%; text-align: center; display: block;">Actions' distribution per frame of Dreamer-V3 agent in the "Dead or Alive++" game</image_caption>
</div>

This can be an useful insight for the developers, indicating that the game's dynamic can be improved.

#### Minecraft

Minecraft is a 3D, first-person, open-world game centered around the gathering of resources and creation of structures and items. It can be played in a single-player mode or a multi-player mode, where all players exist in and affect the same world. Games are played across many sessions, for tens of hours total, per player. Notably, the procedurally generated world is composed of discrete blocks which allow modification; over the course of gameplay, players change their surroundings by gathering resources (such as wood from trees) and constructing structures (such as shelter and storage).  

As an open-world game, Minecraft has no single definable objective. Instead, players develop their own subgoals which form a multitude of natural hierarchies. Though these hierarchies can be exploited, their size and complexity contribute to Minecraft’s inherent difficulty. One such hierarchy is that of item collection: for a large number of objectives in Minecraft, players must create specific tools, materials, and items which require the collection of a strict set of requisite items. The aggregate of these dependencies forms a large-scale task hierarchy.  

In addition to obtaining items, implicit hierarchies emerge through other aspects of gameplay. For example, players (1) construct structures to provide safety for themselves and their stored resources from naturally occurring enemies and (2) explore the world in search of natural resources, often engaging in combat with non-player characters. Both of these gameplay elements have long time horizons and exhibit flexible hierarchies due to situation dependent requirements (such as farming a certain resource necessary to survive, enabling exploration to then gather another resource, and so on).  

We have trained a Dreamer-V3 agent in the [navigate task of MineRL-v0.4.4](https://minerl.readthedocs.io/en/v0.4.4/environments/index.html#minerlnavigatedense-v0), where the agent must move to a goal location denoted by a diamond block. The guidance comes from a “compass” observation, which points near the goal location, 64 meters from the start location. The goal has a small random horizontal offset from the compass location and may be slightly below surface level.

<div class="video-wrapper">
    <video muted autoplay loop controls>
        <source src="/assets/videos/sheepRL_dreamerV3_minecraft_navigate.mp4" type="video/mp4" />
    </video>
    <image_caption style="margin-bottom:28px; width: 100%; text-align: center; display: block;">Dreamer-V3 agent looking for a diamond</image_caption>
</div>

### Conclusion

In summary, SheepRL represents a promising addition to the ever-growing landscape of RL frameworks. Its support for diverse action spaces, dedication to staying at the forefront of RL research, distributed capabilities through Lightning Fabric, and user-friendly design via Hydra make it an exciting choice for researchers and developers seeking to harness the potential of reinforcement learning. As the field continues to evolve, SheepRL is poised to play a valuable role in shaping the future of AI and machine learning. We eagerly anticipate further developments and innovations from the SheepRL team.