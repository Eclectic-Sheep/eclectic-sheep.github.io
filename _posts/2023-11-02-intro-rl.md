---
title: "An introduction to Reinforcement Learning with SheepRL"
author: Federico
excerpt: "Reinforcement Learning is a powerful tool for solving complex problems. In this article we will introduce you Reinforcement Learning, with a focus on the Advantage Actor Critic (A2C) algorithm, and we will show you how to use SheepRL to train an agent in different environments."
tags:
published: true
layout: post
image_header: contain
current: post
cover: assets/images/sheeprl.svg
navigation: True
class: post-template
subclass: 'post'
---

### What is Reinforcement Learning?

Reinforcement Learning (RL) is a branch of Machine Learning that deals with the problem of learning from experience, i.e. "RL is learning what to do - how to map situations to actions - so to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them." (Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction, 2018). Those actions may not only affect the immediate reward but also the next ones and, through that, all subsequent rewards. These two concepts, **trial-and-error** and **delayed reward**, are the two most important distinguishing features of RL, setting it apart from other form of learning, in fact:

* RL is different from **supervised learning** because the training data is not given in the form of input-output pairs, but instead, the agent must discover which actions yield the most reward by trying them
* RL is different from **unsupervised learning**, which is tipically used to uncover hidden structures in unlabeled data, even though one may think of the former as an instance of the latter since RL does not rely on labelled examples of correct behaviour

This informal idea of the agentâ€™s goal being to maximize the total amount of reward it receives is called **reward hypothesis**:

>That all of what we mean by goals and purposes can be well thought of as
the maximization of the expected value of the cumulative sum of a received
scalar signal (called reward).

### RL at a high-level

At a high-level RL can be summarized by the following figure:

<img id="fig-drm" src="{{site.baseurl}}/assets/images/rl_high_level.png" style="margin-bottom: 10px;" />

where the main components are:

* **Agent**: the one that behaves in the environment and learns from experiences gathered through the repeated interactions with it
* **Environment**: the world (physical or virtual) where the agent lives and acts
* **State**: a representation of the environment at a particular point in time, used by the agent to base its decisions
* **Reward**: a numerical value that represents whether the action $A_t$ taken at time $t$ in state $S_t$, resulting in the state $S_{t+1}$, was a good or bad choice

The agent and the environment interact with each other in a sequence of discrete time steps, $t=0,1,2,3,...$. At each time step $t$, the agent receives a representation of the environment's state, $S_t \in \mathcal{S}$, where $\mathcal{S}$ is the set of all possible states. Based on that, the agent selects an action, $A_t \in \mathcal{A}(S_t)$, where $\mathcal{A}(S_t)$ is the set of all possible actions in state $S_t$. One time step later, the agent receives a numerical reward, $R_{t+1} \in \mathcal{R} \subset \mathbb{R}$, and finds the environment in a new state, $S_{t+1}$, changed given the previous agent's action.

The main objective of the agent is to maximize the expected reward over the long run, i.e. to maximize the expected sum of the rewards it receives from the beginning to the end of the (possibly never-ending) episode, $T$ ($\infty$). The expected sum of rewards is called **return** and it is defined as:

$$
G = R_{1} + R_{2} + R_{3} + ... = \sum_{k=0}^{\infty} R_{k+1}
$$

In practice, the reward $R_t$ received at time $t$ is often discounted by a factor $\gamma \in (0,1)$, where $\gamma$ is called **discount rate**, i.e.:

$$
G = R_{1} + \gamma R_{2} + \gamma^2 R_{3} + ... = \sum_{k=0}^{\infty} \gamma^{k} R_{k+1}
$$

This is useful for at least two reasons:

* It allows to make the sum of rewards finite even if the episode is infinite. In fact, if we suppose to have an infinite episode with a constant reward $R_t = r \ll \infty$, then the return would be infinite as well. By discounting the reward, the return becomes:
  $$G = r + \gamma r + \gamma^2 r + ... = \sum_{k=0}^{\infty} \gamma^{k} r = \frac{r}{1-\gamma}$$
* It allows to choose how much importance is given to immediate rewards vs future rewards: the higher the discount rate, the more the agent will care about future rewards; the lower the discount rate, the more the agent will care about immediate rewards instead

### RL at a low-level

At a low-level (i.e. mathematically), RL can be described by the following:

* The set of possible **states** $\mathcal{S}$, where $S_t \in \mathcal{S}$ is the state of the environment at time $t$
* The set of possible **actions** $\mathcal{A}$, where $A_t \in \mathcal{A}$ is the action taken by the agent at time $t$, given the environment state $S_t$
* The **transition function** $\mathcal{P}$, where $$p(s'\vert s,a) : \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \left[0,1\right] = \mathbb{P}\left[S_{t+1} = s' \vert S_t = s, A_t = a \right]$$ is the probability of transitioning to state $s'$ at time $t+1$ given that the agent is in state $s$ at time $t$ and takes action $a$ at time $t$
* The **reward function** $\mathcal{R}$, where $$r(s,a) : \mathcal{S} \times \mathcal{A} \rightarrow \left[0,1\right] = \mathbb{E}\left[ R_{t+1} \vert S_t = s, A_t = a \right]$$ is the expected reward received by the agent at time $t+1$ given that the agent is in state $s$ at time $t$ and takes action $a$ at time $t$
* The **policy** $\pi$, where $\pi(a \vert s) : \mathcal{S} \times \mathcal{A} \rightarrow \left[0,1\right]$ is the probability of taking action $a$ at time $t$ given that the agent is in state $s$ at time $t$

The agent, interacting with the environment, generates a sequence of state-action-reward tuples, called **trajectory**:

$$
\tau = s_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2, r_3, ...
$$

while its goal is to maximize the expected return, averaged over all possible trajectories, i.e. find that policy $\pi$ that maximizes the expected return $\eta(\pi)$:

$$
\eta(\pi) = \mathbb{E}_{s_0, a_0, ...} \left[ \sum_{t=0}^{\infty} \gamma^{t} r(s_t, a_t) \right]
$$

where:

* $s_0$ is a random initial environment state
* $a_t \sim \pi(a_t \vert s_t)$
* $s_{t+1} \sim p(s_{t+1} \vert s_t,a_t)$

### RL at a practical level