---
title: "An introduction to Reinforcement Learning with SheepRL"
author: Federico
excerpt: "Reinforcement Learning is a powerful tool for solving complex problems. In this article we will introduce you Reinforcement Learning, with a focus on the Advantage Actor Critic (A2C) algorithm, and we will show you how to use SheepRL to train an agent in different environments."
tags:
published: false
layout: post
image_header: contain
current: post
cover: assets/images/sheeprl.svg
navigation: True
class: post-template
subclass: 'post'
---

### What is Reinforcement Learning?

Reinforcement Learning (RL) is a branch of Machine Learning that deals with the problem of learning from experience, i.e. "RL is learning what to do - how to map situations to actions - so to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them." (Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction, 2018). Those actions may not only affect the immediate reward but also the next ones and, through that, all subsequent rewards. These two concepts, **trial-and-error** and **delayed reward**, are the two most important distinguishing features of RL. In particular:

* RL is different from **supervised learning** because the training data is not given in the form of input-output pairs, but instead, the agent must discover which actions yield the most reward by trying them
* RL is different from **unsupervised learning**, which is tipically used to uncover hidden structures in unlabeled data, even though one may think of the former as an instance of the latter since RL does not rely on labelled examples of correct behaviour

#### RL at a high-level

At a high-level RL can be summarized by the following figure:

<!--TODO: add figure-->

where the main components are:

* **Agent**: the one that behaves in the environment and learns from experiences gathered through the repeated interactions with it
* **Environment**: the world (physical or virtual) where the agent lives and acts
* **State**: a representation of the environment at a particular point in time, used by the agent to base its decisions
* **Reward**: a numerical value that represents whether the action $A_t$ taken at time $t$ in state $S_t$, resulting in the state $S_{t+1}$, was a good or bad choice

The agent and the environment interact with each other in a sequence of discrete time steps, $t=0,1,2,3,...$. At each time step $t$, the agent receives a representation of the environment's state, $S_t \in \mathcal{S}$, where $\mathcal{S}$ is the set of all possible states. Based on that, the agent selects an action, $A_t \in \mathcal{A}(S_t)$, where $\mathcal{A}(S_t)$ is the set of all possible actions in state $S_t$. One time step later, the agent receives a numerical reward, $R_{t+1} \in \mathcal{R} \subset \mathbb{R}$, and finds the environment in a new state, $S_{t+1}$, changed given the previous agent's action.

